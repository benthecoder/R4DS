---
title: Website redesign AB Testing
subtitle: Data Camp Competition
author: Benedict Neo
date: "Last compiled on `r format(Sys.time(), '%d/%m/%y')`"
output:
  html_document:
   toc: true
   toc_float: true
   toc_depth: 3
   theme: journal
   highlight: zenburn
   df_print: paged
---


# Which version of the website should you use?

## ðŸ“– Background

You work for an early-stage startup in Germany. Your team has been working on a redesign of the landing page. The team believes a new design will increase the number of people who click through and join your site.

They have been testing the changes for a few weeks, and now they want to measure the impact of the change and need you to determine if the increase can be due to random chance or if it is statistically significant.

## ðŸ’¾ The data

The team assembled the following file:

### Redesign test data

-   `treatment` - "yes" if the user saw the new version of the landing page, no otherwise.
-   `new_images` - "yes" if the page used a new set of images, no otherwise.
-   `converted` - 1 if the user joined the site, 0 otherwise.

The control group is those users with "no" in both columns: the old version with the old set of images.

[More about this challenge](https://app.datacamp.com/learn/competitions/webpage-redesign-test)

## Primer on A/B testing

A/B testing is a randomized experiment with two variants A and B. It includes the application of statistical hypothesis testing (two-sample inference).

A/B testing is commonly used to test new products or new features. The main principle is to split users into two groups: control and treatment. Then, we evaluate how users respond and decide which version is better.

For our case, we are testing whether the new version of the landing page, and the new set of images is worth adding and is actually improving conversion.

An important step of an A/B testing is to formulate the hypothesis, it's commonly done as follows

-   Null hypothesis : assumes that the treatments are equal and any difference between the control and experiment groups is due to chance.
-   Alternative hypothesis assumes that the null hypothesis is wrong and the outcomes of control and experiment groups are more different than what chance might produce.

```{r include=FALSE}
knitr::opts_chunk$set(comment = NA) # remove "#" in output
```

## Load Libraries

```{r message=F, warning=F}
# install.packages("kableExtra")
# install.packages("glue")
# install.packages("formattable")
library(tidyverse)
library(kableExtra)
library(glue)
library(ggthemr)
library(broom)
library(formattable)
```

## Load Data

```{r message = FALSE}
df <- readr::read_csv('redesign.csv')
head(df) %>%
    kbl() %>%
    kable_paper("hover", full_width = F)
```

## Analyzing conversion rate for each four groups

There are two changes to the website - landing page and pictures

This gives us four groups of users
- A : old landing and old pictures (control)
- B : new landing and old pictures
- C : old landing and new pictures
- D : new landing and new pictures

What is conversion rate?

Conversion rate is defined as the total number of conversions divided by the number of visitors.

We can either calculate it manually with R, or use the `prop.table` function in R.

We'll also be calculating the uplift for our conversion rates. Lift is calculated as the percent increase/decrease in each metric for users who received a new campaign versus a control group.

With the 4 groups of users, we can calculate the conversion rate for each of them like below


```{r}
A_conv <- df %>%
  filter(treatment == 'no' & new_images == 'no') %>%
  summarize('conversion rate' = round(sum(converted) / nrow(.), 5)) 

glue("Conversion rate for group A is {A_conv}")
```

To prevent repeating code and to add more info to the output, I've written up a function to do that

```{r}
get_info <- function(treatment_bool, new_images_bool, digits = 5) {
  data <- df %>%
    filter(treatment == treatment_bool & new_images == new_images_bool) %>%
    summarize(total_users = nrow(.),
              total_conv = sum(converted),
              conv_rate = round(sum(converted) / nrow(.), digits))
  conv_rate <- data$conv_rate
  total_users <- data$total_users
  total_conv <- data$total_conv
  
  glue("
       Group: treament = {treatment_bool}, new_images = {new_images_bool}
       Total users: {total_users}
       Total conversions: {total_conv}
       Conversion rate : {conv_rate}
       
       ")
}
```


```{r echo=FALSE}
get_info('no', 'no')
get_info('no', 'yes')
get_info('yes', 'no')
get_info('yes', 'yes')
```
### Uplift

Lift is calculated as the percent increase/decrease in each metric for users who received a new campaign versus a control group.

The formula for uplift is ((new_conv_rate - old_conv_rate ) / old_conv_rate ) * 100


```{r}
A_conv <- 0.1071
B_conv <- 0.11254
C_conv <- 0.12005
D_conv <- 0.11372

AB_uplift <- round(((B_conv - A_conv) / A_conv) * 100, 3)
AC_uplift <- round(((C_conv - A_conv) / A_conv) * 100, 3)
BD_uplift <- round(((D_conv - B_conv) / B_conv) * 100, 3)
CD_uplift <- round(((D_conv - C_conv) / C_conv) * 100, 3)
AD_uplift <- round(((D_conv - A_conv) / A_conv) * 100, 3)
```

```{r include=F}
glue("
    A : old landing and old pictures (control)
    B : new landing and old pictures
    C : old landing and new pictures
    D : new landing and new pictures

    AB: For users in group A, adding a new landing page increase the conversion rate from {A_conv} to {B_conv}, and the uplift is {AB_uplift}%
    
    AC: For users in group A, adding a new pictures increase the conversion rate from {A_conv} to {C_conv}, and the uplift is {AC_uplift}%
    
    AD: For users in group A, adding a new pictures and landing page increase the conversion rate from {A_conv} to {D_conv}, and the uplift is {AD_uplift}%
    
    BD: For users in group B, adding a new pictures increase the conversion rate from {B_conv} to {D_conv}, and the uplift is {BD_uplift}%
    
    CD: For users in group C, adding a new landing page decreases the conversion rate from {C_conv} to {D_conv}, and the uplift is {CD_uplift}%
    
     ")
```


To quantify whether this increase/decrease observed is due to random chance or if it is statistically significant, we perform hypothesis testing

## Hypothesis test on design and images

Question statement: Can the increases observed be explained by randomness? 

To answer this question, we carry out Fisher's hypothesis testing, where the hypothesis statements are:

H0: There is no difference for conversion rate for two websites (W1 and W2) 
Ha: There is a difference for conversion rate for two websites (W1 and W2) 

Test statistics = Absolute difference between the conversion rates for websites (W1- W2)


```{r}
A <- df %>% 
  filter(treatment == 'no' & new_images == 'no') %>% 
  select('converted')

B <- df %>% 
  filter(treatment == 'yes' & new_images == 'no') %>% 
  select('converted')

C <- df %>% 
  filter(treatment == 'no' & new_images == 'yes') %>% 
  select('converted')

D <- df %>% 
  filter(treatment == 'yes' & new_images == 'yes') %>% 
  select('converted')
```


```{r include=FALSE}
library("htmltools")
library("webshot")    

export_formattable <- function(f, file, width = "100%", height = NULL, 
                               background = "white", delay = 0.2)
    {
      w <- as.htmlwidget(f, width = width, height = height)
      path <- html_print(w, background = background, viewer = NULL)
      url <- paste0("file:///", gsub("\\\\", "/", normalizePath(path)))
      webshot(url,
              file = file,
              selector = ".formattable_widget",
              delay = delay)
}

## FT <- formattable(data)
## export_formattable(FT, "file_name.png")
```

```{r echo=F}
sign_formatter <- formatter("span", 
  style = x ~ style(color = ifelse(x >= 0.05, "red", "green")))

formatter_tile = formatter(
  "span",
  style = x ~ style(
    color = ifelse(abs(x) < mean(abs(x)) / 2, "grey", "white"),
    display = "block",
    padding = "0 4px",
    `border-radius` = "4px",
    `background-color` = ifelse(x == 1,
                                "transparent",
                                csscolor(
                                  gradient(as.numeric(abs(x)), "lightgreen", "darkgreen")
                                ))
  )
)

bold <- formatter("span", 
  style = x ~ style("font-weight" = "bold"))


# more on https://renkun-ken.github.io/formattable/articles/formattable-data-frame.html
```


```{r}
library(formattable)
out <- bind_rows(tidy(t.test(A, B)),
                 tidy(t.test(A, C)),
                 tidy(t.test(B, D)),
                 tidy(t.test(C, D)),
                 tidy(t.test(A, D)),
                 .id = "test",) %>%
  as_tibble() %>%
  rename(
    "diff_mean" = estimate,
    "t-statistic" = statistic,
  ) %>%
  select(test:p.value, -c(estimate1, estimate2)) %>%
  mutate_if(is.numeric, round, 4)

# rename rows
out$test <- c("A - B", "A - C", "B - D", "C - D", "A - D")

# formatters are custom, find them in code
formattable(out, list(
  test = bold,
  p.value = sign_formatter,
  diff_mean = formatter_tile,
  `t-statistic` = formatter_tile)) 
```

## Conclusion

Based on the table above, there is only one test that is statistically significant, and that is group A to B test. The p-value is 0.0037 < 0.05. For the rest of the test, they are not significant. The test with the highest p-value was adding a new landing page for a design with new images.

So to answer the question of which version of the website the company should use - the answer, based on statistics, is to implement the design of a new landing page without the old set of images.

## Resource

- [Learning Data Science: A/B Testing in Under One Minute](https://www.r-bloggers.com/2020/06/learning-data-science-a-b-testing-in-under-one-minute/)
- [A Guide to A/B Testing â€” How to Formulate, Design and Interpret | by Idil Ismiguzel](https://towardsdatascience.com/a-guide-to-a-b-testing-how-to-formulate-design-and-interpret-f820cc62e21a)
- [A/B Testing: A Complete Guide to Statistical Testing | by Francesco Casalegno | Towards Data Science](https://towardsdatascience.com/a-b-testing-a-complete-guide-to-statistical-testing-e3f1db140499)
- [A/B Testing in R. What is A/B Testing? | by Sheenal Srivastava | Towards Data Science](https://towardsdatascience.com/a-b-testing-in-r-ae819ce30656)

## Other reports
- [Bayesian A/B testing](https://app.datacamp.com/workspace/w/47fb0ef8-ea01-4f36-aa88-2f2547b37d27)
